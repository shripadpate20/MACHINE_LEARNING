# -*- coding: utf-8 -*-
"""ANN_From_Scratch.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LlaKHjW8iIM5besopnMLPu6TuKZKKF14
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import OneHotEncoder
from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt

data=pd.read_csv("data.csv")

y_data= data['label']
x_data=data.drop(columns=['label'])

"""#### one hot encoding"""

ohe=OneHotEncoder(sparse=True)
encoded=y_data.values.reshape(len(y_data),1)
labels=ohe.fit_transform(encoded).toarray()

"""#### train test split"""

from sklearn.model_selection import train_test_split

"""# for 70:30 train test split"""

x_train, x_test, y_train, y_test = train_test_split(x_data, labels, test_size=0.3, random_state=2)

print(x_train.shape)
print(x_test.shape)
print(y_train.shape)
print(y_test.shape)

x_train=x_train/255
x_test=x_test/255

x_train = np.array(x_train)
x_test  = np.array(x_test)

x_train=x_train.T
x_test=x_test.T
y_train=y_train.T
y_test=y_test.T

print(x_train.shape)
print(x_test.shape)
print(y_train.shape)
print(y_test.shape)

"""#### activation function and derivative"""

def sigmoid(z):
    a=1/(1+np.exp(-z))
    return a

def softmax(z):
    a=np.exp(z)/(np.sum(np.exp(z),axis=0))
    return a

def sigmoid_derivative(z):
    return sigmoid(z) * (1 - sigmoid(z))

"""#### initialize parameters"""

def initialize_parameters(layer):

    L=len(layer)-1
    parameters={}
    for l in range(1,L+1):
        np.random.seed(7)
        parameters['W' + str(l)] = np.random.randn(layer[l], layer[l-1]) *0.75
        parameters['b' + str(l)] = np.ones((layer[l],1))

    return parameters
#initialize_parameters([784,128,64,32,10])

layer = [784,128,64,32,10]
params = initialize_parameters(layer)

for l in range(1, len(layer)):
    print("Shape of W" + str(l) + ":", params['W' + str(l)].shape)
    print("Shape of B" + str(l) + ":", params['b' + str(l)].shape, "\n")

"""#### forward propagation"""

def forward_propagation(x,parameters):
    forward_values={}
    L=len(parameters)//2
    forward_values['h0'] = x
    for l in range(1,L):
        forward_values['a' + str(l)] =np.dot( parameters['W' + str(l)],forward_values['h' + str(l-1)])+ parameters['b' + str(l)]

        forward_values['h' + str(l)] = sigmoid(forward_values['a' + str(l)])
    forward_values['a' + str(L)] = parameters['W' + str(L)].dot(forward_values['h' + str(L-1)]) + parameters['b' + str(L)]
    forward_values['h' + str(L)] = softmax(forward_values['a' + str(L)])
    return forward_values['h' + str(L)], forward_values



aL,forw_values= forward_propagation(x_train, params)

for l in range(len(params)//2 + 1):
    print("Shape of h" + str(l) + " :", forw_values['h' + str(l)].shape)

def compute_cost(hL, Y):
    m = Y.shape[1]

    loss = -(1./m) * np.sum(Y*np.log(hL))

    loss = np.squeeze(loss)

    return loss

"""#### backward propagation"""

def backward_propagation(hL, Y, parameters, forward_values):

    derivative = {}
    L = len(parameters)//2
    m = hL.shape[1]

    derivative["da" + str(L)] = hL - Y
    derivative["dW" + str(L)] = 1./m * np.dot(derivative["da" + str(L)],forward_values['h' + str(L-1)].T)
    derivative["db" + str(L)] = 1./m * np.sum(derivative["da" + str(L)], axis = 1)
    derivative["db" + str(L)] = np.array( derivative["db" + str(L)])
    derivative["db" + str(L)] = derivative["db" + str(L)].reshape(-1, 1)
    for l in reversed(range(1, L)):
         derivative["da" + str(l)] = np.dot(parameters['W' + str(l+1)].T,derivative["da" + str(l+1)])*sigmoid_derivative(forward_values["h" + str(l)])
         derivative["dW" + str(l)] = 1./m * np.dot(derivative["da" + str(l)],forward_values['h' + str(l-1)].T)
         derivative["db" + str(l)] = 1./m * np.sum(derivative["da" + str(l)], axis = 1)
         derivative["db" + str(l)] = np.array( derivative["db" + str(l)])
         derivative["db" + str(l)] = derivative["db" + str(l)].reshape(-1, 1)


    return derivative

derivative = backward_propagation(forw_values["h" + str(4)], y_train, params, forw_values)

for l in reversed(range(1, len(derivative)//3 + 1)):
    print("Shape of da" + str(l) + " :", derivative['da' + str(l)].shape)
    print("Shape of dW" + str(l) + " :", derivative['dW' + str(l)].shape)
    print("Shape of dB" + str(l) + " :", derivative['db' + str(l)].shape, "\n")

"""#### update parameters"""

def update_parameters(parameters, derivative, learning_rate):

    L = len(parameters) // 2

    for l in range(L):
        parameters["W" + str(l+1)] = parameters["W" + str(l+1)] - learning_rate * derivative["dW" + str(l+1)]
        parameters["b" + str(l+1)] = parameters["b" + str(l+1)] - learning_rate * derivative["db" + str(l+1)]

    return parameters

def accuracy(x, y, parameters):

    m = x.shape[1]
    y_pred,_ = forward_propagation(x, parameters)
    y = np.argmax(y, 0)
    y_pred = np.argmax(y_pred, 0)

    return np.round(np.sum((y_pred == y)/m), 3)

"""### train the model"""

def train(X, y, layer,epochs, learning_rate, batch_size):
    m = X.shape[1]
    num_batches = m // batch_size
    parameters = initialize_parameters(layer)
    train_losses = []
    test_losses = []
    train_accuracies = []
    test_accuracies = []


    for epoch in range(epochs):
        for i in range(0, m - batch_size + 1, batch_size):
            x_batch = x_train[:, i:i+batch_size]
            y_batch = y_train[:, i:i+batch_size]

            hL, forward_values = forward_propagation(x_batch, parameters)

            derivative = backward_propagation(hL, y_batch, parameters, forward_values)

            parameters = update_parameters(parameters, derivative, learning_rate)
        loss = compute_cost(hL, y_batch)
        accuracy_train=accuracy(x_train, y_train, parameters)
        accuracy_test=accuracy(x_test, y_test, parameters)
        print(" \t loss: {} \t train_acc:{} \t test_acc:{}".format( np.round(loss, 3), accuracy(x_train, y_train, parameters), accuracy(x_test, y_test, parameters)))
        train_accuracies.append(accuracy_train)
        test_accuracies.append(accuracy_test)
        train_losses.append(loss)
        test_losses.append(loss)

    return parameters,train_accuracies, test_accuracies,train_losses,test_losses

layer= [784,128,64,32,10]
parameters,train_accuracies, test_accuracies,train_losses,test_losses =train(x_train, y_train, layer,epochs=25, learning_rate=0.0034, batch_size=23)

def plot(train_accuracies,test_accuracies,train_losses,test_losses,epochs):
    plt.figure(figsize=(12, 6))
    plt.subplot(1, 2, 1)
    plt.plot(range(1, epochs + 1), train_accuracies, label='Train Accuracy')
    plt.plot(range(1, epochs + 1), test_accuracies, label='Test Accuracy')
    plt.title('Accuracy per Epoch')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.legend()

    plt.subplot(1, 2, 2)
    plt.plot(range(1, epochs + 1), train_losses, label='Train Loss')
    plt.plot(range(1, epochs + 1), test_losses, label='Test Loss')
    plt.title('Loss per Epoch')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()

    plt.tight_layout()
    plt.show()

plot(train_accuracies,test_accuracies,train_losses,test_losses,epochs=25)

y_pred,_ = forward_propagation(x_train, parameters)
y = np.argmax(y_train, 0)
y_pred = np.argmax(y_pred, 0)
conf_matrix = confusion_matrix(y,y_pred)
print("\nConfusion Matrix:")
print(conf_matrix)

"""# for 70:20 train test split"""

y_data= data['label']
x_data=data.drop(columns=['label'])

x_train, x_test, y_train, y_test = train_test_split(x_data, labels, test_size=0.2, random_state=2)

print(x_train.shape)
print(x_test.shape)
print(y_train.shape)
print(y_test.shape)

x_train=x_train/255
x_test=x_test/255

x_train = np.array(x_train)
x_test  = np.array(x_test)

x_train=x_train.T
x_test=x_test.T
y_train=y_train.T
y_test=y_test.T

print(x_train.shape)
print(x_test.shape)
print(y_train.shape)
print(y_test.shape)

layer= [784,128,64,32,10]
parameters,train_accuracies, test_accuracies,train_losses,test_losses =train(x_train, y_train, layer,epochs=25, learning_rate=0.0034, batch_size=23)

plot(train_accuracies,test_accuracies,train_losses,test_losses,epochs=25)

y_pred,_ = forward_propagation(x_train, parameters)
y = np.argmax(y_train, 0)
y_pred = np.argmax(y_pred, 0)
conf_matrix = confusion_matrix(y,y_pred)
print("\nConfusion Matrix:")
print(conf_matrix)

"""# for 70:10 train test split"""

y_data= data['label']
x_data=data.drop(columns=['label'])

x_train, x_test, y_train, y_test = train_test_split(x_data, labels, test_size=0.1, random_state=2)

print(x_train.shape)
print(x_test.shape)
print(y_train.shape)
print(y_test.shape)

x_train=x_train/255
x_test=x_test/255

x_train = np.array(x_train)
x_test  = np.array(x_test)

x_train=x_train.T
x_test=x_test.T
y_train=y_train.T
y_test=y_test.T

print(x_train.shape)
print(x_test.shape)
print(y_train.shape)
print(y_test.shape)

layer= [784,128,64,32,10]
parameters,train_accuracies, test_accuracies,train_losses,test_losses =train(x_train, y_train, layer,epochs=25, learning_rate=0.0034, batch_size=23)

plot(train_accuracies,test_accuracies,train_losses,test_losses,epochs=25)

y_pred,_ = forward_propagation(x_train, parameters)
y = np.argmax(y_train, 0)
y_pred = np.argmax(y_pred, 0)
conf_matrix = confusion_matrix(y,y_pred)
print("\nConfusion Matrix:")
print(conf_matrix)

